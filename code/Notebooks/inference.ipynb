{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae244db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hotson/kaggle_work/recodai-luc-scientific-image-forgery-detection/code\n"
     ]
    }
   ],
   "source": [
    "%cd \"/home/hotson/kaggle_work/recodai-luc-scientific-image-forgery-detection/code\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b3f89",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd24fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import cv2  # type: ignore\n",
    "except Exception:  # pragma: no cover - optional dependency\n",
    "    cv2 = None\n",
    "\n",
    "from utils import evaluate_single_image, rle_encode, score as competition_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40977b12",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ef7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, img_size: int = 448, processor=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_size = img_size\n",
    "        self.processor = processor\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            self.to_tensor,\n",
    "            self.normalize,\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        case_id = row[\"case_id\"]\n",
    "        mask_path = row.get(\"mask_path\", \"\")\n",
    "        label = int(row.get(\"label\", 0))\n",
    "        image_path = row[\"image_path\"]\n",
    "\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            img = Image.open(f).convert(\"RGB\")\n",
    "\n",
    "        orig_size = img.size  # (W,H)\n",
    "\n",
    "        if self.processor is not None:\n",
    "            img_rs = transforms.functional.resize(img, (self.img_size, self.img_size))\n",
    "            processed = self.processor(\n",
    "                images=img_rs,\n",
    "                return_tensors=\"pt\",\n",
    "                do_resize=False,\n",
    "                do_center_crop=False,\n",
    "                do_normalize=True,\n",
    "                do_rescale=True,\n",
    "            )\n",
    "            img_t = processed[\"pixel_values\"].squeeze(0)\n",
    "        else:\n",
    "            img_t = self.img_transform(img)\n",
    "\n",
    "        return img_t, case_id, orig_size, mask_path, image_path, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2bb23",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb3c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_case_id(raw) -> int | str:\n",
    "    if torch.is_tensor(raw):\n",
    "        raw = raw.item()\n",
    "    return int(raw) if isinstance(raw, (np.integer, int)) else str(raw)\n",
    "\n",
    "\n",
    "def enhanced_adaptive_mask(prob: np.ndarray, alpha_grad=0.45):\n",
    "    prob = np.asarray(prob, dtype=np.float32)\n",
    "    if prob.ndim == 3:\n",
    "        prob = prob[..., 0]\n",
    "\n",
    "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
    "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
    "\n",
    "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
    "    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
    "\n",
    "    thr = float(np.mean(enhanced) + 0.3 * np.std(enhanced))\n",
    "    mask = (enhanced > thr).astype(np.uint8)\n",
    "\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
    "    return mask, thr\n",
    "\n",
    "\n",
    "def finalize_mask(prob: np.ndarray, orig_size_wh: tuple[int, int]):\n",
    "    # orig_size_wh MUST be (W,H)\n",
    "    mask, thr = enhanced_adaptive_mask(prob)\n",
    "    w, h = orig_size_wh\n",
    "    mask = cv2.resize(mask, (int(w), int(h)), interpolation=cv2.INTER_NEAREST)\n",
    "    return mask, thr\n",
    "\n",
    "\n",
    "def apply_overlay(img_np: np.ndarray, mask01: np.ndarray, color=(255, 0, 0), alpha=0.5):\n",
    "    overlay = img_np.copy()\n",
    "    mask_bool = mask01.astype(bool)\n",
    "    overlay[mask_bool] = (\n",
    "        alpha * np.array(color, dtype=np.float32)\n",
    "        + (1 - alpha) * overlay[mask_bool].astype(np.float32)\n",
    "    ).astype(np.uint8)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, case_ids, orig_sizes, mask_paths, image_paths, labels = zip(*batch)\n",
    "    images = default_collate(images)\n",
    "    return images, list(case_ids), list(orig_sizes), list(mask_paths), list(image_paths), list(labels)\n",
    "\n",
    "\n",
    "def infer_img_size_from_state(state_dict: dict, fallback: int) -> int:\n",
    "    pos_embed = state_dict.get(\"backbone.pos_embed\")\n",
    "    patch_weight = state_dict.get(\"backbone.patch_embed.proj.weight\")\n",
    "    if pos_embed is None or patch_weight is None:\n",
    "        return fallback\n",
    "\n",
    "    tokens = pos_embed.shape[1]\n",
    "    grid_tokens = tokens - 1\n",
    "    grid = int(math.sqrt(grid_tokens))\n",
    "    if grid * grid != grid_tokens:\n",
    "        return fallback\n",
    "\n",
    "    patch = int(patch_weight.shape[-1])\n",
    "    return grid * patch\n",
    "\n",
    "\n",
    "def create_collage(\n",
    "    image_path: str,\n",
    "    pred_bin: np.ndarray,\n",
    "    case_id: str,\n",
    "    score: float,\n",
    "    out_dir: Path,\n",
    "    gt_label: str,\n",
    "):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        orig_img = Image.open(f).convert(\"RGB\")\n",
    "    orig_np = np.array(orig_img)\n",
    "    vis_w, vis_h = orig_img.size  # (W,H)\n",
    "\n",
    "    if pred_bin.shape[:2] != (vis_h, vis_w):\n",
    "        pred_bin_vis = cv2.resize(pred_bin, (vis_w, vis_h), interpolation=cv2.INTER_NEAREST)\n",
    "    else:\n",
    "        pred_bin_vis = pred_bin\n",
    "\n",
    "    gt_mask = load_gt_mask_from_image_path(image_path = image_path, w = vis_w, h = vis_h)\n",
    "\n",
    "    if gt_mask.shape[:2] != (vis_h, vis_w):\n",
    "        gt_mask_vis = cv2.resize(gt_mask, (vis_w, vis_h), interpolation=cv2.INTER_NEAREST)\n",
    "    else:\n",
    "        gt_mask_vis = gt_mask\n",
    "\n",
    "    pred_mask_rgb = (pred_bin_vis[..., None] * 255).repeat(3, axis=2).astype(np.uint8)\n",
    "    gt_mask_rgb = (gt_mask_vis[..., None] * 255).repeat(3, axis=2).astype(np.uint8)\n",
    "\n",
    "    pred_overlay = apply_overlay(orig_np, pred_bin_vis, color=(255, 0, 0))\n",
    "    gt_overlay = apply_overlay(orig_np, gt_mask_vis, color=(0, 255, 0))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "    titles = [f\"Input_{case_id}\", \"Pred mask\", \"GT mask\", \"Pred overlay\", \"GT overlay\"]\n",
    "    imgs = [orig_np, pred_mask_rgb, gt_mask_rgb, pred_overlay, gt_overlay]\n",
    "    for ax, img, title in zip(axes, imgs, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    score_str = f\"{score:.3f}\".replace(\".\", \"pt\")\n",
    "    save_path = out_dir / f\"{gt_label}_{case_id}_{score_str}.png\"\n",
    "    plt.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def load_gt_mask_from_image_path(image_path: str, w: int, h: int) -> np.ndarray:\n",
    "    mask_path = (\n",
    "        image_path\n",
    "        .replace(\"train_images\", \"train_masks\")\n",
    "        .replace(\"supplemental_images\", \"supplemental_masks\")\n",
    "        .replace(\"/forged\", \"\")\n",
    "        .replace(\".png\", \".npy\")\n",
    "    )\n",
    "    if not Path(mask_path).exists():\n",
    "        return np.zeros(shape = (h, w), dtype = np.uint8)\n",
    "    \n",
    "    arr = np.load(mask_path, allow_pickle=True)\n",
    "    if arr.ndim == 3:\n",
    "        arr = arr.max(axis=0)\n",
    "    gt_mask = (arr > 0).astype(np.uint8)\n",
    "    if gt_mask.shape[:2] != (h, w):\n",
    "        gt_mask = cv2.resize(gt_mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    return gt_mask\n",
    "\n",
    "\n",
    "def save_hist(values, title, out_path: Path, bins=50):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(values, bins=bins)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf62f4",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8792ad",
   "metadata": {},
   "source": [
    "#### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf513dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    csv_path = \"../analysis/area_splits/val_fold0.csv\"\n",
    "    weights_path = \"../exps/20260110_131204/vit_base_patch14_dinov2.lvd142m_fold0.pt\"\n",
    "    arch: str = \"dinov2_uperhead\"\n",
    "    dinov2_id: str = \"facebook/dinov2-base\"\n",
    "    model_name: str = \"vit_base_patch14_dinov2.lvd142m\"\n",
    "    img_size: int = 532\n",
    "    use_hf_processor: bool = True\n",
    "    outdir: Path = \"../analysis/preds_nposw\"\n",
    "    save_collages: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e3f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding suffix\n",
    "outdir = args.outdir + f\"_{args.arch}\"\n",
    "outdir = Path(outdir)\n",
    "if outdir.exists():\n",
    "    shutil.rmtree(outdir)\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(args.csv_path)\n",
    "\n",
    "hf_processor = None\n",
    "if args.use_hf_processor:\n",
    "    from transformers import AutoImageProcessor\n",
    "    hf_processor = AutoImageProcessor.from_pretrained(args.dinov2_id)\n",
    "\n",
    "weights_path = Path(args.weights_path)\n",
    "if not weights_path.exists():\n",
    "    raise FileNotFoundError(f\"No weights file at: {weights_path}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_collages = args.save_collages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d899e01",
   "metadata": {},
   "source": [
    "#### Collecting model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d559453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting masks: 100%|██████████| 513/513 [00:36<00:00, 13.87it/s]\n"
     ]
    }
   ],
   "source": [
    "state = torch.load(weights_path, map_location=device)\n",
    "model_state = state[\"model_state\"]\n",
    "img_size = args.img_size\n",
    "\n",
    "if args.arch == \"dino_seg\":\n",
    "    inferred_img_size = infer_img_size_from_state(model_state, img_size)\n",
    "    if inferred_img_size != img_size:\n",
    "        print(f\"Adjusting img_size from {img_size} to {inferred_img_size} based on checkpoint pos_embed.\")\n",
    "        img_size = inferred_img_size\n",
    "else:\n",
    "    if img_size % 14 != 0:\n",
    "        raise ValueError(f\"{args.arch} requires img_size divisible by 14 (Dinov2 patch size).\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset / loader\n",
    "# -------------------------\n",
    "dataset = InferenceDataset(df, img_size=img_size, processor=hf_processor)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "if args.arch == \"dino_seg\":\n",
    "    from dinov2_seg import DinoSegModel\n",
    "    model = DinoSegModel(model_name=args.model_name, pretrained=False, img_size=img_size).to(device)\n",
    "elif args.arch == \"dinov2_uperhead\":\n",
    "    from dinov2_uperhead import DinoV2_UPerNet\n",
    "    model = DinoV2_UPerNet(dinov2_id=args.dinov2_id, num_classes=1).to(device)\n",
    "elif args.arch == \"dinov2_unet\":\n",
    "    from dinov2_unet import DinoV2UNet\n",
    "    model = DinoV2UNet(dinov2_id=args.dinov2_id, out_classes=1).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown arch: {args.arch}\")\n",
    "\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "print(\"[INFO] Loaded model\")\n",
    "\n",
    "# -------------------------\n",
    "# Run inference once and cache per-image stats (for grid + final submission)\n",
    "# -------------------------\n",
    "predictions = []\n",
    "solution_rows = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, case_ids, orig_sizes, mask_paths, image_paths, labels in tqdm(loader, desc=\"Predicting masks\"):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        out = model(images)\n",
    "        logits = out if torch.is_tensor(out) else out[0]\n",
    "        probs = torch.sigmoid(logits).float().cpu().numpy()  # (B,1,H,W)\n",
    "\n",
    "        for i, case_id in enumerate(case_ids):\n",
    "            case_id_val = str(normalize_case_id(case_id))\n",
    "            orig_w, orig_h = orig_sizes[i]\n",
    "            prob = probs[i, 0]  # (img_size,img_size)\n",
    "\n",
    "            mask_full, thr = finalize_mask(prob, (orig_w, orig_h))  # mask in orig size (H,W)\n",
    "\n",
    "            gt_mask = load_gt_mask_from_image_path(image_paths[i], orig_w, orig_h)\n",
    "            gt_label = \"authentic\" if int(labels[i]) == 0 else \"forged\"\n",
    "            shape_str = json.dumps([int(orig_h), int(orig_w)])  # (H,W) for scorer\n",
    "\n",
    "            predictions.append(dict(\n",
    "                case_id=case_id_val,\n",
    "                image_path=image_paths[i],\n",
    "                orig_width=int(orig_w),\n",
    "                orig_height=int(orig_h),\n",
    "                prob=prob,\n",
    "                pred_mask=mask_full.astype(np.uint8),\n",
    "                gt_mask=gt_mask.astype(np.uint8),\n",
    "                gt_label=gt_label,\n",
    "                thr=float(thr),\n",
    "                shape_str=shape_str,\n",
    "            ))\n",
    "\n",
    "            # scorer needs 'annotation' and 'shape' columns in solution\n",
    "            solution_rows.append(dict(\n",
    "                case_id=case_id_val,\n",
    "                annotation=\"authentic\" if gt_label == \"authentic\" else rle_encode([gt_mask.astype(np.uint8)]),\n",
    "                shape=shape_str,\n",
    "            ))\n",
    "\n",
    "solution_df = pd.DataFrame(solution_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb210623",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1edd380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing CC variants:   0%|          | 0/1026 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing CC variants: 100%|██████████| 1026/1026 [00:21<00:00, 47.86it/s]\n",
      "Grid Search (CC=0): 100%|██████████| 140/140 [02:40<00:00,  1.15s/it, A=900, CC=0, M=0.75, best=0.523972]\n",
      "Grid Search (CC=50): 100%|██████████| 140/140 [02:41<00:00,  1.15s/it, A=100, CC=50, M=0.55, best=0.525308]\n",
      "Grid Search (CC=100): 100%|██████████| 140/140 [02:42<00:00,  1.16s/it, A=100, CC=100, M=0.55, best=0.530177]\n",
      "Grid Search (CC=200): 100%|██████████| 140/140 [02:41<00:00,  1.15s/it]\n",
      "Grid Search (CC=400): 100%|██████████| 140/140 [02:39<00:00,  1.14s/it]\n",
      "Grid Search (CC=800): 100%|██████████| 140/140 [02:40<00:00,  1.15s/it, A=100, CC=800, M=0.75, best=0.532601]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST: 0.5326009045463622 {'area_thres': 100, 'mean_thres': 0.75, 'min_cc_area': 800}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# FAST Grid Search (precompute CC variants once)\n",
    "# Replace your whole slow triple-loop block with this.\n",
    "# Assumes you already have:\n",
    "#   - predictions: list[dict] with keys: case_id, pred_mask (H,W), prob (S,S)\n",
    "#   - solution_df\n",
    "#   - competition_score(...)\n",
    "#   - rle_encode(...)\n",
    "#   - cv2 imported\n",
    "# -------------------------\n",
    "\n",
    "area_range = [i * 100 for i in range(1, 11)]\n",
    "mean_range = [round(x, 2) for x in np.arange(0.20, 0.9, 0.05)]\n",
    "cc_range = [0, 50, 100, 200, 400, 800]\n",
    "\n",
    "best_score = -1.0\n",
    "best_params = {\"area_thres\": None, \"mean_thres\": None, \"min_cc_area\": None}\n",
    "\n",
    "\n",
    "def remove_small_components(mask01: np.ndarray, min_area: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    mask01: uint8/bool (H,W) with values {0,1}\n",
    "    min_area: remove connected components smaller than this many pixels\n",
    "    \"\"\"\n",
    "    if min_area is None or min_area <= 0:\n",
    "        return (mask01 > 0).astype(np.uint8)\n",
    "\n",
    "    m = (mask01 > 0).astype(np.uint8)\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(m, connectivity=8)\n",
    "\n",
    "    out = np.zeros_like(m, dtype=np.uint8)\n",
    "    for lab in range(1, num_labels):\n",
    "        if stats[lab, cv2.CC_STAT_AREA] >= min_area:\n",
    "            out[labels == lab] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) Precompute per-image stats/RLE for each CC threshold\n",
    "# -------------------------\n",
    "N = len(predictions)\n",
    "K = len(cc_range)\n",
    "\n",
    "area_cc = np.zeros((N, K), dtype=np.int32)\n",
    "mean_cc = np.zeros((N, K), dtype=np.float32)\n",
    "rle_cc = [[None] * K for _ in range(N)]  # precomputed string per (image, cc)\n",
    "\n",
    "for i, pred in enumerate(tqdm(predictions, desc=\"Precomputing CC variants\", leave=True)):\n",
    "    prob = pred[\"prob\"]  # (S,S) model-res prob map\n",
    "    base = (pred[\"pred_mask\"] > 0).astype(np.uint8)  # (H,W) bin mask in orig size\n",
    "\n",
    "    # We'll reuse the same resize target each time\n",
    "    prob_w, prob_h = prob.shape[1], prob.shape[0]\n",
    "\n",
    "    for k, min_cc in enumerate(cc_range):\n",
    "        m = remove_small_components(base, int(min_cc))\n",
    "        a = int(m.sum())\n",
    "        area_cc[i, k] = a\n",
    "\n",
    "        if a > 0:\n",
    "            m_small = cv2.resize(m, (prob_w, prob_h), interpolation=cv2.INTER_NEAREST)\n",
    "            mean_cc[i, k] = float(prob[m_small == 1].mean()) if (m_small == 1).any() else 0.0\n",
    "            rle_cc[i][k] = rle_encode([m])  # NOTE: RLE of CC-filtered mask\n",
    "        else:\n",
    "            mean_cc[i, k] = 0.0\n",
    "            rle_cc[i][k] = \"authentic\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) Grid search without cv2 / RLE inside the hot loop\n",
    "# -------------------------\n",
    "case_ids = [p[\"case_id\"] for p in predictions]\n",
    "\n",
    "for k, min_cc in enumerate(cc_range):\n",
    "    combos_2d = list(itertools.product(area_range, mean_range))\n",
    "    pbar = tqdm(combos_2d, desc=f\"Grid Search (CC={min_cc})\", leave=True)\n",
    "\n",
    "    for a_thr, m_thr in pbar:\n",
    "        is_forged = (area_cc[:, k] >= a_thr) & (mean_cc[:, k] >= m_thr)\n",
    "\n",
    "        submission_rows = [\n",
    "            {\n",
    "                \"case_id\": case_ids[i],\n",
    "                \"annotation\": (rle_cc[i][k] if is_forged[i] else \"authentic\"),\n",
    "            }\n",
    "            for i in range(N)\n",
    "        ]\n",
    "\n",
    "        submission_df = pd.DataFrame(submission_rows)\n",
    "        curr_score = competition_score(\n",
    "            solution=solution_df,\n",
    "            submission=submission_df,\n",
    "            row_id_column_name=\"case_id\",\n",
    "        )\n",
    "\n",
    "        if curr_score > best_score:\n",
    "            best_score = float(curr_score)\n",
    "            best_params = {\n",
    "                \"area_thres\": int(a_thr),\n",
    "                \"mean_thres\": float(m_thr),\n",
    "                \"min_cc_area\": int(min_cc),\n",
    "            }\n",
    "            pbar.set_postfix(best=f\"{best_score:.6f}\", A=a_thr, M=m_thr, CC=min_cc)\n",
    "\n",
    "print(\"BEST:\", best_score, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c3b19",
   "metadata": {},
   "source": [
    "#### Submission run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "473df059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final submission: 100%|██████████| 1026/1026 [09:00<00:00,  1.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved: ../analysis/preds_nposw_dinov2_uperhead/submission.csv\n",
      "[INFO] Saved: ../analysis/preds_nposw_dinov2_uperhead/analysis/run_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# -------- final submission + optional collages\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "analysis_dir = outdir / \"analysis\"\n",
    "analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "collage_dir = outdir / \"collage\"\n",
    "if save_collages:\n",
    "    collage_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# find cc index\n",
    "best_cc = int(best_params[\"min_cc_area\"])\n",
    "best_k = cc_range.index(best_cc)\n",
    "\n",
    "final_rows = []\n",
    "# For hists (use best CC filtered values)\n",
    "best_area_vals = area_cc[:, best_k].tolist()\n",
    "best_mean_vals = mean_cc[:, best_k].tolist()\n",
    "thr_vals = [p[\"thr\"] for p in predictions]\n",
    "\n",
    "for i, p in enumerate(tqdm(predictions, desc=\"Final submission\", leave=True)):\n",
    "    a_thr = int(best_params[\"area_thres\"])\n",
    "    m_thr = float(best_params[\"mean_thres\"])\n",
    "\n",
    "    is_forged = (area_cc[i, best_k] >= a_thr) and (mean_cc[i, best_k] >= m_thr)\n",
    "    annotation = (rle_cc[i][best_k] if is_forged else \"authentic\")\n",
    "    final_rows.append({\"case_id\": p[\"case_id\"], \"annotation\": annotation})\n",
    "\n",
    "    if save_collages:\n",
    "        # compute per-image score (only for naming)\n",
    "        label_rles = solution_df.loc[solution_df[\"case_id\"] == p[\"case_id\"], \"annotation\"].iloc[0]\n",
    "        pred_rles = annotation\n",
    "\n",
    "        if (label_rles == \"authentic\") or (pred_rles == \"authentic\"):\n",
    "            image_score = 1.0 if (label_rles == pred_rles) else 0.0\n",
    "        else:\n",
    "            image_score = float(evaluate_single_image(\n",
    "                label_rles=label_rles,\n",
    "                prediction_rles=pred_rles,\n",
    "                shape_str=p[\"shape_str\"],\n",
    "            ))\n",
    "\n",
    "        # for visualization, decode the chosen pred mask\n",
    "        if annotation == \"authentic\":\n",
    "            final_mask = np.zeros_like(p[\"gt_mask\"], dtype=np.uint8)\n",
    "        else:\n",
    "            # We already have CC-filtered binary mask inside RLE, but not stored as array.\n",
    "            # Recreate it quickly: apply CC filter again (only for collage, not for scoring)\n",
    "            base = (p[\"pred_mask\"] > 0).astype(np.uint8)\n",
    "            final_mask = remove_small_components(base, best_cc)\n",
    "\n",
    "        create_collage(\n",
    "            image_path=p[\"image_path\"],\n",
    "            pred_bin=final_mask,\n",
    "            case_id=p[\"case_id\"],\n",
    "            score=image_score,\n",
    "            out_dir=collage_dir,\n",
    "            gt_label=p[\"gt_label\"],\n",
    "        )\n",
    "\n",
    "final_submission_df = pd.DataFrame(final_rows)\n",
    "submission_path = outdir / \"submission.csv\"\n",
    "final_submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"[INFO] Saved: {submission_path}\")\n",
    "\n",
    "# -------- distributions\n",
    "save_hist(best_area_vals, f\"Mask Area (CC>={best_cc})\", analysis_dir / \"mask_area_hist.png\")\n",
    "save_hist(best_mean_vals, f\"Mask Mean-Inside (CC>={best_cc})\", analysis_dir / \"mask_mean_inside_hist.png\")\n",
    "save_hist(thr_vals, \"Finalize Threshold (thr)\", analysis_dir / \"thr_hist.png\")\n",
    "\n",
    "# -------- metadata json\n",
    "run_meta = {\n",
    "    \"best_score\": best_score,\n",
    "    \"best_params\": best_params,\n",
    "    \"per_case\": [\n",
    "        {\n",
    "            \"case_id\": p[\"case_id\"],\n",
    "            \"image_path\": p[\"image_path\"],\n",
    "            \"orig_width\": p[\"orig_width\"],\n",
    "            \"orig_height\": p[\"orig_height\"],\n",
    "            \"gt_label\": p[\"gt_label\"],\n",
    "            \"thr\": p[\"thr\"],\n",
    "            \"area_cc_best\": int(area_cc[i, best_k]),\n",
    "            \"mean_cc_best\": float(mean_cc[i, best_k]),\n",
    "        }\n",
    "        for i, p in enumerate(predictions)\n",
    "    ],\n",
    "}\n",
    "meta_path = analysis_dir / \"run_metadata.json\"\n",
    "meta_path.write_text(json.dumps(run_meta, indent=2))\n",
    "print(f\"[INFO] Saved: {meta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c45c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7319bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
